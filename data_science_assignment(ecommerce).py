# -*- coding: utf-8 -*-
"""Data Science Assignment(eCommerce).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iwe-MOKw_4Jt8Tip_PUrzB_NDUSJu8pX
"""

#1. Exploratory Data Analysis (EDA)
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load datasets
customers = pd.read_csv('Customers.csv')
products = pd.read_csv('Products.csv')
transactions = pd.read_csv('Transactions.csv')

# Display basic information
print(customers.head())
print(products.head())
print(transactions.head())

# Merging transactions with products and customers for combined analysis
data = transactions.merge(products, on='ProductID').merge(customers, on='CustomerID')

data

# Checking for missing values
print(data.isnull().sum())

# Top 5 most purchased products
top_products = data.groupby('ProductName')['Quantity'].sum().sort_values(ascending=False).head(5)
print(top_products)

# Top 5 customers by total transaction value
top_customers = data.groupby('CustomerName')['TotalValue'].sum().sort_values(ascending=False).head(5)
print(top_customers)

# Visualization: Distribution of TotalValue
plt.figure(figsize=(8, 5))
sns.histplot(data['TotalValue'], bins=30, kde=True)
plt.title("Distribution of Transaction Values")
plt.xlabel("TotalValue")
plt.ylabel("Frequency")
plt.show()

#2 .Lookalike Model
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# Combine customer profiles and transaction data
customer_profiles = data.groupby('CustomerID').agg({
    'ProductName': lambda x: ' '.join(x),
    'TotalValue': 'sum'
}).reset_index()

customer_profiles

# Use TF-IDF for text similarity
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(customer_profiles['ProductName'])

# Compute cosine similarity
similarity_matrix = cosine_similarity(tfidf_matrix)

# Find top 3 similar customers for the first 20 customers
lookalikes = {}
for i, customer_id in enumerate(customer_profiles['CustomerID'][:20]):
    similar_indices = np.argsort(similarity_matrix[i])[::-1][1:4]  # Exclude itself
    similar_customers = [(customer_profiles.iloc[j]['CustomerID'], similarity_matrix[i][j]) for j in similar_indices]
    lookalikes[customer_id] = similar_customers

# Save results to Lookalike.csv
lookalike_df = pd.DataFrame([
    {'CustomerID': customer, 'SimilarCustomers': str(lookalikes[customer])} for customer in lookalikes
])
lookalike_df.to_csv('Lookalike.csv', index=False)

lookalike_df

#3. Customer Segmentation / Clustering
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import davies_bouldin_score

# Prepare data for clustering
features = customer_profiles[['TotalValue']]
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

features

scaler

scaled_features

# Apply KMeans clustering
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(scaled_features)

kmeans

clusters

# Add clusters to the dataset
customer_profiles['Cluster'] = clusters

# Calculate Davies-Bouldin Index
db_index = davies_bouldin_score(scaled_features, clusters)
print(f"Davies-Bouldin Index: {db_index}")

# Visualize clusters
plt.figure(figsize=(8, 5))
sns.scatterplot(x=scaled_features[:, 0], y=scaled_features[:, 0], hue=clusters, palette='viridis')
plt.title("Customer Segments")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

